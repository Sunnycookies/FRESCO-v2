{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Zero-Shot Video Translation and Editing with Frame Spatial-Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) preperation\n",
    "\n",
    "- download the full repository\n",
    "- prepare environment on `requirements.txt`\n",
    "- run `python install.py` to download the supporting models\n",
    "  - check `./model/gmflow_sintel-0c07dcb3.pth` and `epoch_resnet.pth` are downloaded\n",
    "  - check `./src/ControlNet/annotator/ckpts/ControlNetHED.pth` is downloaded\n",
    "  - check `./src/ControlNet/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt` is downloaded\n",
    "  - check `./src/ebsynth/deps/ebsynth/bin/ebsynth (or ebsynth.exe)` is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"6\"\n",
    "\n",
    "# uncomment the next line to use huggingface model in China\n",
    "#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, DDPMScheduler, ControlNetModel, DDIMScheduler\n",
    "\n",
    "from src.utils import *\n",
    "from src.tokenflow_utils import *\n",
    "from src.keyframe_selection import get_keyframe_ind, get_keyframe_ind_extend\n",
    "from src.diffusion_hacked import apply_FRESCO_attn, apply_FRESCO_opt, register_conv_control_efficient\n",
    "from src.pipe_FRESCO import inference\n",
    "from run_fresco import check_config, apply_control, run_full_video_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) define suppporting functions: load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(config):\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('creating models...')\n",
    "    import sys\n",
    "    sys.path.append(\"./src/ebsynth/deps/gmflow/\")\n",
    "    sys.path.append(\"./src/EGNet/\")\n",
    "    sys.path.append(\"./src/ControlNet/\")\n",
    "    \n",
    "    from gmflow.gmflow import GMFlow\n",
    "    from model import build_model\n",
    "    from annotator.hed import HEDdetector\n",
    "    from annotator.canny import CannyDetector\n",
    "    from annotator.midas import MidasDetector\n",
    "\n",
    "    # optical flow\n",
    "    flow_model = GMFlow(feature_channels=128,\n",
    "                   num_scales=1,\n",
    "                   upsample_factor=8,\n",
    "                   num_head=1,\n",
    "                   attention_type='swin',\n",
    "                   ffn_dim_expansion=4,\n",
    "                   num_transformer_layers=6,\n",
    "                   ).to('cuda')\n",
    "    \n",
    "    local_files_only = False\n",
    "    \n",
    "    checkpoint = torch.load(config['gmflow_path'], map_location=lambda storage, loc: storage)\n",
    "    weights = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
    "    flow_model.load_state_dict(weights, strict=False)\n",
    "    flow_model.eval() \n",
    "    print('create optical flow estimation model successfully!')\n",
    "    \n",
    "    # saliency detection\n",
    "    sod_model = build_model('resnet')\n",
    "    sod_model.load_state_dict(torch.load(config['sod_path']))\n",
    "    sod_model.to(\"cuda\").eval()\n",
    "    print('create saliency detection model successfully!')\n",
    "    \n",
    "    # controlnet\n",
    "    if config['controlnet_type'] not in ['hed', 'depth', 'canny']:\n",
    "        print('unsupported control type, set to hed')\n",
    "        config['controlnet_type'] = 'hed'\n",
    "    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-\"+config['controlnet_type'], \n",
    "                                                 torch_dtype=torch.float16, local_files_only=local_files_only)\n",
    "    controlnet.to(\"cuda\") \n",
    "    if config['controlnet_type'] == 'depth':\n",
    "        detector = MidasDetector()\n",
    "    elif config['controlnet_type'] == 'canny':\n",
    "        detector = CannyDetector()\n",
    "    else:\n",
    "        detector = HEDdetector()\n",
    "    print('create controlnet model-' + config['controlnet_type'] + ' successfully!')\n",
    "    \n",
    "    # diffusion model\n",
    "    if config['sd_path'] == 'stabilityai/stable-diffusion-2-1-base':\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(config['sd_path'], torch_dtype=torch.float16, local_files_only=local_files_only)\n",
    "    else:\n",
    "        vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16, local_files_only=local_files_only)\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(config['sd_path'], vae=vae, torch_dtype=torch.float16, local_files_only=local_files_only)\n",
    "    if config['edit_mode'] == 'SDEdit':\n",
    "        pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "    else:\n",
    "        pipe.scheduler = DDIMScheduler.from_pretrained(config['sd_path'], subfolder=\"scheduler\", local_files_only=local_files_only)\n",
    "    pipe.to(\"cuda\")\n",
    "    pipe.scheduler.set_timesteps(config['num_inference_steps'], device=pipe._execution_device)\n",
    "    \n",
    "    if config['use_freeu']:\n",
    "        from src.free_lunch_utils import apply_freeu\n",
    "        apply_freeu(pipe, b1=1.2, b2=1.5, s1=1.0, s2=1.0)\n",
    "\n",
    "    frescoProc = apply_FRESCO_attn(pipe, config['use_inversion'], True, config['edit_mode'])\n",
    "    frescoProc.controller.disable_controller()\n",
    "    apply_FRESCO_opt(pipe, use_inversion=config['use_inversion'])\n",
    "    print('create diffusion model ' + config['sd_path'] + ' successfully!')\n",
    "    \n",
    "    for param in flow_model.parameters():\n",
    "        param.requires_grad = False    \n",
    "    for param in sod_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in controlnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in pipe.unet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return pipe, frescoProc, controlnet, detector, flow_model, sod_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) set configurations\n",
    "- I/O of files, models, etc.\n",
    "    - file_path: path to the video\n",
    "    - save_path: path to save the results \n",
    "    - sd_path: path to the stable diffusion model on huggingface\n",
    "    - gmflow_path: path to the optical flow estimation model\n",
    "    - sod_path: path to the saliency detection model\n",
    "    - inv_save_path: path where inversion latents are saved\n",
    "\n",
    "- Method selection\n",
    "    - use_fresco: whether use FRESCO\n",
    "    - edit_mode: keyframe editing mode (pnp, SDEdit)\n",
    "    - synth_mode: non-keyframe synthesis mode (Tokenflow, ebsynth, None, Mixed)\n",
    "    - use_controlnet: whether use ControlNet\n",
    "    - controlnet_type: ControlNet type (hed, depth, canny)\n",
    "    - use_freeu: whether use FreeU to enhance output details\n",
    "    - use_salinecy: whether use background smoothing\n",
    "    - use_inversion: whether use DDIM inversion\n",
    "\n",
    "- Keyframe selection\n",
    "    - keyframe_select_mode: keyframe selecting mode (loop, fixed)\n",
    "    - keyframe_select_radix: keyframe selecting radix when using loop mode\n",
    "    - mininterv: min keyframe interval \n",
    "    - maxinterv: max keyframe interval \n",
    "\n",
    "- Video translation\n",
    "    - prompt: prompt for the video translation\n",
    "    - seed: seed\n",
    "    - batch_size: how many frames in a batch (set lower to prevent OOM)\n",
    "    - num_inference_steps: total step\n",
    "\n",
    "- Frame manipulation\n",
    "    - num_intraattn_steps: intra-frame attention end step\n",
    "    - end_opt_step: feature optimization steps from num_warmup_steps to end_opt_step\n",
    "    - cond_scale: ControlNet strength\n",
    "    - num_warmup_steps: SDEdit begin step\n",
    "    - pnp_attn_t: pnp attentions injection begin step\n",
    "    - pnp_f_t: pnp features injection begin step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config/ref_config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['synth_mode'] = 'Tokenflow'\n",
    "config['edit_mode'] = 'pnp'\n",
    "config['file_path'] = 'data/bread.mp4'\n",
    "config['save_path'] = 'result/bread'\n",
    "config['keyframe_select_mode'] = 'loop'\n",
    "config['keyframe_select_radix'] = 6\n",
    "config['use_inversion'] = True\n",
    "config['inv_latent_path'] = 'result/latents/bread/inv_step_500/latents'\n",
    "config['mininterv'] = config['maxinterv'] = 4\n",
    "config['use_fresco'] = True\n",
    "config['prompt'] = 'an ice sculpture'\n",
    "config['batch_size'] = 4\n",
    "config['num_intraattn_steps'] = 0\n",
    "config['sd_path'] = 'stabilityai/stable-diffusion-2-1-base'\n",
    "\n",
    "run_ebsynth, run_tokenflow = check_config(config)\n",
    "\n",
    "for name, value in sorted(config.items()):\n",
    "    print('%s: %s' % (str(name), str(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) select primary keyframes (optional) and keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 100)\n",
    "print('key frame selection for \\\"%s\\\"...'%(config['file_path']))\n",
    "\n",
    "video_cap = cv2.VideoCapture(config['file_path'])\n",
    "frame_num = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "if config['primary_select']:\n",
    "    primary_indexes = get_keyframe_ind(config['file_path'], mininterv=config['mininterv'], maxinterv=config['maxinterv'])\n",
    "    print(f'choose primary indexes {primary_indexes}')\n",
    "else:\n",
    "    primary_indexes = list(range(frame_num))\n",
    "primary_indexes_posmap = {primary_indexes[i]:i for i in range(len(primary_indexes))}\n",
    "keys = get_keyframe_ind_extend(config['file_path'], config['keyframe_select_mode'], config['keyframe_select_radix'], \n",
    "                               primary_indexes, config['mininterv'], config['maxinterv'])\n",
    "sublists_all = []\n",
    "for ind in range(len(keys)):\n",
    "    sublists = [keys[ind][i:i+config['batch_size']-2] for i in range(2, len(keys[ind]), config['batch_size']-2)]\n",
    "    sublists[0].insert(0, keys[ind][0])\n",
    "    sublists[0].insert(1, keys[ind][1])\n",
    "    while len(sublists_all) < len(sublists):\n",
    "        sublists_all.append([])\n",
    "    for batch_ind, keys_batch in enumerate(sublists):\n",
    "        sublists_all[batch_ind].append(keys_batch)\n",
    "print(f\"split keyframes into groups {sublists_all}\")\n",
    "\n",
    "keylists = []\n",
    "for keys_group in sublists_all:\n",
    "    keys_all = []\n",
    "    for key in keys_group:\n",
    "        keys_all += key\n",
    "    # keylists.append(list(np.unique(keys_all)))\n",
    "    keylists.append(keys_all)\n",
    "print(f\"split keyframes into batches {keylists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) load model and prepare for keyframe translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, frescoProc, controlnet, detector, flow_model, sod_model = get_models(config)\n",
    "device = pipe._execution_device\n",
    "guidance_scale = 7.5\n",
    "do_classifier_free_guidance = guidance_scale > 1\n",
    "assert(do_classifier_free_guidance)\n",
    "timesteps = pipe.scheduler.timesteps\n",
    "cond_scale = [config['cond_scale']] * config['num_inference_steps']\n",
    "dilate = Dilate(device=device)\n",
    "\n",
    "base_prompt = config['prompt']\n",
    "if 'n_prompt' in config and 'a_prompt' in config:\n",
    "    a_prompt = config['a_prompt']\n",
    "    n_prompt = config['n_prompt']\n",
    "elif 'Realistic' in config['sd_path'] or 'realistic' in config['sd_path']:\n",
    "    a_prompt = ', RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3, '\n",
    "    n_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation'\n",
    "else:\n",
    "    a_prompt = ', best quality, extremely detailed, '\n",
    "    n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing finger, extra digit, fewer digits, cropped, worst quality, low quality'    \n",
    "\n",
    "# you can set extra_prompts for individual keyframe\n",
    "# for example, extra_prompts[38] = ', closed eyes' to specify the person frame38 closes the eyes\n",
    "extra_prompts = [''] * frame_num\n",
    "\n",
    "os.makedirs(config['save_path'], exist_ok=True)\n",
    "if os.path.exists(os.path.join(config['save_path'], 'keys')):\n",
    "    os.system(f\"rm -rf {os.path.join(config['save_path'], 'keys')}\")\n",
    "os.makedirs(os.path.join(config['save_path'], 'keys'))\n",
    "if run_ebsynth:\n",
    "    os.makedirs(os.path.join(config['save_path'], 'video'), exist_ok=True)\n",
    "\n",
    "if config['edit_mode']=='pnp':\n",
    "    pnp_attn_t = int(config[\"num_inference_steps\"] * config[\"pnp_attn_t\"])\n",
    "    qk_injection_timesteps = pipe.scheduler.timesteps[:pnp_attn_t] if pnp_attn_t >= 0 else []\n",
    "    frescoProc.controller.set_qk_injection_timesteps(qk_injection_timesteps)\n",
    "    \n",
    "    pnp_f_t = int(config[\"num_inference_steps\"] * config[\"pnp_f_t\"])\n",
    "    conv_injection_timesteps = pipe.scheduler.timesteps[:pnp_f_t] if pnp_f_t >= 0 else []\n",
    "    register_conv_control_efficient(pipe, conv_injection_timesteps)\n",
    "\n",
    "batch_ind = 0\n",
    "imgs = []\n",
    "img_idx = []\n",
    "record_latent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) run keyframe translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "video_cap = cv2.VideoCapture(config['file_path'])\n",
    "\n",
    "print(config['prompt'])\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('video to video translation...')\n",
    "\n",
    "for i in range(frame_num):\n",
    "    success, frame = video_cap.read()\n",
    "    if success == False:\n",
    "        print(f\"{'/' * 100}\\nAn error occurred when reading frame from {config['file_path']} frame {i}\\n{'/' * 100}\")\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = resize_image(frame, 512)\n",
    "    if run_ebsynth:\n",
    "        Image.fromarray(img).save(os.path.join(config['save_path'], 'video/%04d.png'%(i)))\n",
    "    \n",
    "    if i not in primary_indexes:\n",
    "        continue\n",
    "\n",
    "    if i not in keylists[batch_ind] and not run_tokenflow:\n",
    "        continue\n",
    "\n",
    "    imgs += [img]\n",
    "    img_idx += [i]\n",
    "    \n",
    "    if batch_ind < len(keylists) - 1:\n",
    "        if i != keylists[batch_ind][-1]:\n",
    "            continue\n",
    "    elif run_tokenflow:\n",
    "        if i != primary_indexes[-1]:\n",
    "            continue\n",
    "    elif i != keylists[batch_ind][-1]:\n",
    "        continue\n",
    "\n",
    "    print(f\"processing batch [{batch_ind + 1}/{len(keylists)}] with images {img_idx}\")\n",
    "\n",
    "    propagation_mode = batch_ind > 0\n",
    "\n",
    "    prompts = [base_prompt + a_prompt + extra_prompts[ind] for ind in img_idx]\n",
    "    if propagation_mode:\n",
    "        if run_tokenflow:\n",
    "            assert len(img_idx) == primary_indexes_posmap[img_idx[-1]] - primary_indexes_posmap[keylists[batch_ind - 1][-1]] + 2\n",
    "        else:\n",
    "            assert len(img_idx) == len(keylists[batch_ind]) + 2\n",
    "        prompts = ref_prompts + prompts\n",
    "    inv_prompts = [config['inv_prompt']] * len(img_idx)\n",
    "\n",
    "    imgs_torch = torch.cat([numpy2tensor(img) for img in imgs], dim=0)\n",
    "    print('input of current batch:')\n",
    "    viz = torchvision.utils.make_grid(imgs_torch, 6, 1)\n",
    "    visualize(viz.cpu(), 160)\n",
    "\n",
    "    edges = torch.cat([numpy2tensor(apply_control(img, detector, config)[:, :, None]) for img in imgs], dim=0)\n",
    "    edges = edges.repeat(1,3,1,1).cuda() * 0.5 + 0.5\n",
    "    if do_classifier_free_guidance:\n",
    "        edges = torch.cat([edges.to(pipe.unet.dtype)] * 2)\n",
    "\n",
    "    pos_map = {img_idx[i]:i for i in range(len(img_idx))}\n",
    "    prefix = [0, 1] if propagation_mode else []\n",
    "    keylists_pos = [prefix + [pos_map[key] for key in keygroup] + ([pos_map[img_idx[-1]]] if img_idx[-1] not in keygroup else [])\n",
    "                    for keygroup in sublists_all[batch_ind]]\n",
    "\n",
    "    print(f\"keyframe indexes of images {sublists_all[batch_ind]}\")\n",
    "    print(f\"keyframe indexes of position {keylists_pos}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.autocast(dtype=torch.float16, device_type='cuda'):\n",
    "        latents = inference(pipe, controlnet, frescoProc, imgs, edges, timesteps, img_idx, keylists_pos, n_prompt, \n",
    "                            prompts, inv_prompts, config['inv_latent_path'], config['temp_paras_save_path'], \n",
    "                            config['end_opt_step'], propagation_mode, False, config['use_fresco'], do_classifier_free_guidance, \n",
    "                            run_tokenflow, config['edit_mode'], False, config['use_controlnet'], config['use_saliency'], \n",
    "                            config['use_inversion'], cond_scale, config['num_inference_steps'], config['num_warmup_steps'], \n",
    "                            config['seed'], guidance_scale, record_latent, config['num_intraattn_steps'], \n",
    "                            flow_model=flow_model, sod_model=sod_model, dilate=dilate)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(dtype=torch.float16, device_type='cuda'):\n",
    "        start = 2 if propagation_mode else 0\n",
    "        size = len(latents)\n",
    "        image = []\n",
    "        for i in range(start, size, config['batch_size']):\n",
    "            end = min(size, i + config['batch_size'])\n",
    "            image_batch = pipe.vae.decode(latents[i:end] / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image.append(image_batch)\n",
    "        image = torch.cat(image)\n",
    "        image = torch.clamp(image, -1, 1)\n",
    "        print('results of current batch:')\n",
    "        viz = torchvision.utils.make_grid(image, 6, 1)\n",
    "        visualize(viz.cpu(), 160)\n",
    "        save_imgs = tensor2numpy(image)\n",
    "        if run_ebsynth:\n",
    "            for ind, num in enumerate(img_idx[start:]):\n",
    "                Image.fromarray(save_imgs[ind]).save(os.path.join(config['save_path'], 'keys/%04d.png'%(num)))\n",
    "        \n",
    "    batch_ind += 1\n",
    "    imgs = [imgs[0], imgs[-1]]\n",
    "    img_idx = [img_idx[0], img_idx[-1]]\n",
    "    ref_prompts = [prompts[0], prompts[-1]]\n",
    "    if batch_ind == len(keylists):\n",
    "        break\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_tokenflow:\n",
    "    keys = primary_indexes\n",
    "else:\n",
    "    keys = keys[config['num_inference_steps'] % len(keys)]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) run full video translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_full_video_translation(config, keys, run_ebsynth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "release",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
